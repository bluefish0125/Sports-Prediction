{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a015969b-c296-4b92-a0ea-99d0dd5df390",
   "metadata": {},
   "source": [
    "- [1. Instructions](#1)\n",
    "- [2. Importing Functions and Packages](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b002d-7571-40dd-9431-b4061e0d43af",
   "metadata": {},
   "source": [
    "## 1. Instructions <a id='1'></a>\n",
    "- Connect to GitHub in py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3822d1-0412-4bce-b1be-be28d11a7350",
   "metadata": {},
   "source": [
    "## 2. Importing Functions and Packages <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b27480a3-c621-4922-848e-e4ced2ef3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.endpoints import playergamelog, leaguedashteamstats, teamyearbyyearstats\n",
    "from nba_api.stats.library.parameters import SeasonAll\n",
    "from nba_api.stats.static import players, teams\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7dbaf2a-a884-4856-880f-da010a345deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_suffixes(dataframes, names, keys):\n",
    "    \"\"\"Merging different sets of data of the same season together\"\"\"\n",
    "    suffixed_dfs = []\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Suffix non-key columns only\n",
    "        suffixed_cols = {col: f\"{col}_{name}\" if col not in keys else col for col in df.columns}\n",
    "        suffixed_dfs.append(df.rename(columns=suffixed_cols))\n",
    "\n",
    "    merged_df = suffixed_dfs[0]\n",
    "    for df in suffixed_dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=keys, how='inner')\n",
    "    return merged_df\n",
    "    \n",
    "\n",
    "def extracting_today_data(from_date=None, season='2023-24', season_type='Regular Season', data_type='Base', delay=5):\n",
    "    year_start = season[:4]\n",
    "    year_end = str(int(year_start)+1)\n",
    "    season_ = year_start + \"-\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "    if from_date == None:\n",
    "        path_today = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/base_{season}.csv\"\n",
    "        from_date = pd.read_csv(path_today, parse_dates=['Date'])['Date'].max().date()\n",
    "\n",
    "    season_end = datetime.date.today()\n",
    "    \n",
    "    current_date = from_date\n",
    "    all_data = []\n",
    "\n",
    "    unsuccessful_dates = []\n",
    "    \n",
    "    while current_date <= season_end:\n",
    "        date_str = current_date.strftime('%m/%d/%Y')\n",
    "        \n",
    "        try:\n",
    "            daily_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "                measure_type_detailed_defense=data_type,\n",
    "                season=season_,\n",
    "                season_type_all_star=season_type,\n",
    "                date_to_nullable=date_str\n",
    "            ).get_data_frames()[0]\n",
    "            daily_stats['Date'] = date_str\n",
    "            all_data.append(daily_stats)\n",
    "            print(f\"Data fetched for {date_str}\")\n",
    "        except Exception as e:\n",
    "            unsuccessful_dates += [date_str]\n",
    "            print(f\"Error fetching data for {date_str}: {e}\")\n",
    "        time.sleep(delay)\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    full_season_data = pd.concat(all_data, ignore_index=True)\n",
    "    return full_season_data, unsuccessful_dates\n",
    "\n",
    "def getting_stats(delay=5, retrieving_from=None):\n",
    "    \"\"\"Getting cumulative season stats for each game\"\"\"\n",
    "    track_start='2023'\n",
    "    season_end='2024'\n",
    "    if retrieving_from is None:\n",
    "        retrieving_from = datetime.now() - timedelta(seconds=delay)\n",
    "    \n",
    "    base, unsuccessful_dates_base = extracting_today_data(from_date=retrieving_from, data_type='Base', delay=delay)\n",
    "    advanced, unsuccessful_dates_advanced = extracting_today_data(from_date=retrieving_from, data_type='Advanced', delay=delay)\n",
    "    misc, unsuccessful_dates_misc = extracting_today_data(from_date=retrieving_from, data_type='Misc', delay=delay)\n",
    "    four_factors, unsuccessful_dates_four_factors = extracting_today_data(from_date=retrieving_from, data_type='Four Factors', delay=delay)\n",
    "    scoring, unsuccessful_dates_scoring = extracting_today_data(from_date=retrieving_from, data_type='Scoring', delay=delay)\n",
    "    opponent, unsuccessful_dates_opponent = extracting_today_data(from_date=retrieving_from, data_type='Opponent', delay=delay)\n",
    "    defense, unsuccessful_dates_defense = extracting_today_data(from_date=retrieving_from, data_type='Defense', delay=delay)\n",
    "\n",
    "    year_start = track_start[:4]\n",
    "    year_end = season_end[:4]\n",
    "    season_ = year_start + \"_\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "\n",
    "    datas = [base, advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    \n",
    "    unsuccessful_dates_lst = [unsuccessful_dates_base, unsuccessful_dates_advanced, unsuccessful_dates_misc, unsuccessful_dates_four_factors, unsuccessful_dates_scoring, unsuccessful_dates_opponent, unsuccessful_dates_defense]\n",
    "    datas_names = [\"base\", \"advanced\", \"misc\", \"four_factors\",\n",
    "               \"scoring\", \"opponent\", \"defense\"]\n",
    "    columns_to_exclude = ['TEAM_NAME', 'GP', 'W', 'L', 'W_PCT', 'MIN']\n",
    "    others = [advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    others = [i[i.columns[~i.columns.isin(columns_to_exclude)]] for i in others]\n",
    "    datas = [base] + others\n",
    "    \n",
    "    merge_keys = ['Date', 'TEAM_ID']\n",
    "    merged_df = merge_with_suffixes(datas, datas_names, merge_keys)    \n",
    "    latest_path = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/cumulative_season_stats_{season}.csv\"\n",
    "    prev = pd.read_csv(latest_path, parse_dates=['Date'])\n",
    "    merged_df = pd.concat([prev, merged_df], ignore_index=True)\n",
    "    merged_df.to_csv(latest_path, index=False)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa1f7fc3-1ccc-4eb6-92a2-71aeeca3f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4m/qh070ww90rqgmcw604kkdt8r0000gn/T/ipykernel_22758/3101355566.py:22: DtypeWarning: Columns (240) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(StringIO(response.text), parse_dates=['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Your personal access token and repo details\n",
    "token = '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'\n",
    "username = 'bluefish0125'\n",
    "repo = 'Sports-Betting'\n",
    "path_to_file = 'nba_api/data/teams_stats/processed_cum_2018_2024.csv'\n",
    "\n",
    "# GitHub API URL for your file\n",
    "url = f'https://api.github.com/repos/{username}/{repo}/contents/{path_to_file}'\n",
    "\n",
    "# Hvae to use raw file\n",
    "raw_url = 'https://raw.githubusercontent.com/bluefish0125/Sports-Betting/main/nba_api/data/teams_stats/processed_cum_2018_2024.csv'\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.VERSION.raw'\n",
    "}\n",
    "\n",
    "# Send a GET request with headers including your personal access token for authentication\n",
    "response = requests.get(raw_url, headers=headers)\n",
    "if response.status_code == 200: # 200 means successful\n",
    "    df = pd.read_csv(StringIO(response.text), parse_dates=['Date'])\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='mixed')    \n",
    "    max_day = df['Date'].max().date()\n",
    "    next_day = max_day + datetime.timedelta(days=1)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the CSV file. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95ab3fb1-b44e-44d9-a30f-c89bc0597477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 4, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "91260a93-4e9a-4a5e-8848-0532704760f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n",
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n",
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n",
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n",
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n",
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n",
      "Data fetched for 04/03/2024\n",
      "Data fetched for 04/04/2024\n"
     ]
    }
   ],
   "source": [
    "today = getting_stats(retrieving_from=next_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50606416-6e98-4dc7-8ca2-aeab8280e96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to CSV\n",
    "csv_content = df.iloc[0, :].to_csv(index=False)\n",
    "content_encoded = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')\n",
    "new_path_to_file = 'data_pipeline/test_file/test_file_1.csv'\n",
    "new_url = f'https://api.github.com/repos/{username}/{repo}/contents/{new_path_to_file}'\n",
    "# Prepare headers\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "# Fetch the file from GitHub to get its SHA\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "sha = data['sha']\n",
    "\n",
    "# Create the payload with the new content and the SHA\n",
    "payload = {\n",
    "    'message': 'Update CSV file',\n",
    "    'content': content_encoded,\n",
    "    'sha': sha,\n",
    "    'branch': 'main',  # specify the branch if not 'main'\n",
    "}\n",
    "\n",
    "# Make a PUT request to update the file\n",
    "response = requests.put(url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('File updated successfully.')\n",
    "else:\n",
    "    print('Failed to update the file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d07c8fd-656d-4e0f-844f-a92f2fc13913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file created successfully.\n"
     ]
    }
   ],
   "source": [
    "csv_content = df.iloc[0, :].to_csv(index=False)\n",
    "content_encoded = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')\n",
    "new_path_to_file = 'data_pipeline/test_file/test_file_1.csv'\n",
    "new_url = f'https://api.github.com/repos/{username}/{repo}/contents/{new_path_to_file}'\n",
    "\n",
    "# Prepare headers for authentication\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "# Create the payload with the new content\n",
    "payload = {\n",
    "    'message': 'Add new CSV file',\n",
    "    'content': content_encoded,\n",
    "    'branch': 'main',  # specify the branch if not 'main'\n",
    "}\n",
    "\n",
    "# Make a PUT request to upload the file\n",
    "response = requests.put(new_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print('New file created successfully.')\n",
    "else:\n",
    "    print(f'Failed to create the new file. Status Code: {response.status_code} Response: {response.json()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7934f-4b61-4c39-a97c-2d547e15b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.endpoints import playergamelog, leaguedashteamstats, teamyearbyyearstats\n",
    "from nba_api.stats.library.parameters import SeasonAll\n",
    "from nba_api.stats.static import players, teams\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def merge_with_suffixes(dataframes, names, keys):\n",
    "    \"\"\"Merging different sets of data of the same season together\"\"\"\n",
    "    suffixed_dfs = []\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Suffix non-key columns only\n",
    "        suffixed_cols = {col: f\"{col}_{name}\" if col not in keys else col for col in df.columns}\n",
    "        suffixed_dfs.append(df.rename(columns=suffixed_cols))\n",
    "\n",
    "    merged_df = suffixed_dfs[0]\n",
    "    for df in suffixed_dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=keys, how='inner')\n",
    "    return merged_df\n",
    "    \n",
    "\n",
    "def extracting_today_data(from_date=None, season='2023-24', season_type='Regular Season', data_type='Base', delay=5):\n",
    "    year_start = season[:4]\n",
    "    year_end = str(int(year_start)+1)\n",
    "    season_ = year_start + \"-\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "    if from_date == None:\n",
    "        path_today = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/base_{season}.csv\"\n",
    "        from_date = pd.read_csv(path_today, parse_dates=['Date'])['Date'].max().date()\n",
    "\n",
    "    season_end = datetime.date.today()\n",
    "    \n",
    "    current_date = from_date\n",
    "    all_data = []\n",
    "\n",
    "    unsuccessful_dates = []\n",
    "    \n",
    "    while current_date <= season_end:\n",
    "        date_str = current_date.strftime('%m/%d/%Y')\n",
    "        \n",
    "        try:\n",
    "            daily_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "                measure_type_detailed_defense=data_type,\n",
    "                season=season_,\n",
    "                season_type_all_star=season_type,\n",
    "                date_to_nullable=date_str\n",
    "            ).get_data_frames()[0]\n",
    "            daily_stats['Date'] = date_str\n",
    "            all_data.append(daily_stats)\n",
    "            print(f\"Data fetched for {date_str}\")\n",
    "        except Exception as e:\n",
    "            unsuccessful_dates += [date_str]\n",
    "            print(f\"Error fetching data for {date_str}: {e}\")\n",
    "        time.sleep(delay)\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    full_season_data = pd.concat(all_data, ignore_index=True)\n",
    "    return full_season_data, unsuccessful_dates\n",
    "\n",
    "def getting_stats(delay=5):\n",
    "    \"\"\"Getting cumulative season stats for each game\"\"\"\n",
    "    track_start='2023'\n",
    "    season_end='2024'\n",
    "    \n",
    "    base, unsuccessful_dates_base = extracting_today_data(data_type='Base', delay=delay)\n",
    "    advanced, unsuccessful_dates_advanced = extracting_today_data(data_type='Advanced', delay=delay)\n",
    "    misc, unsuccessful_dates_misc = extracting_today_data(data_type='Misc', delay=delay)\n",
    "    four_factors, unsuccessful_dates_four_factors = extracting_today_data(data_type='Four Factors', delay=delay)\n",
    "    scoring, unsuccessful_dates_scoring = extracting_today_data(data_type='Scoring', delay=delay)\n",
    "    opponent, unsuccessful_dates_opponent = extracting_today_data(data_type='Opponent', delay=delay)\n",
    "    defense, unsuccessful_dates_defense = extracting_today_data(data_type='Defense', delay=delay)\n",
    "\n",
    "    year_start = track_start[:4]\n",
    "    year_end = season_end[:4]\n",
    "    season_ = year_start + \"_\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "\n",
    "    datas = [base, advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    unsuccessful_dates_lst = [unsuccessful_dates_base, unsuccessful_dates_advanced, unsuccessful_dates_misc, unsuccessful_dates_four_factors, unsuccessful_dates_scoring, unsuccessful_dates_opponent, unsuccessful_dates_defense]\n",
    "    datas_names = [\"base\", \"advanced\", \"misc\", \"four_factors\",\n",
    "               \"scoring\", \"opponent\", \"defense\"]\n",
    "    columns_to_exclude = ['TEAM_NAME', 'GP', 'W', 'L', 'W_PCT', 'MIN']\n",
    "    others = [advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    others = [i[i.columns[~i.columns.isin(columns_to_exclude)]] for i in others]\n",
    "    datas = [base] + others\n",
    "    \n",
    "    merge_keys = ['Date', 'TEAM_ID']\n",
    "    merged_df = merge_with_suffixes(datas, datas_names, merge_keys)    \n",
    "    latest_path = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/cumulative_season_stats_{season}.csv\"\n",
    "    prev = pd.read_csv(latest_path, parse_dates=['Date'])\n",
    "    merged_df = pd.concat([prev, merged_df], ignore_index=True)\n",
    "    merged_df.to_csv(latest_path, index=False)\n",
    "    return merged_df\n",
    "\n",
    "today = getting_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
