{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a015969b-c296-4b92-a0ea-99d0dd5df390",
   "metadata": {},
   "source": [
    "- [1. Instructions](#1)\n",
    "- [2. Importing Functions and Packages](#2)\n",
    "- [3. Retrieving Datasets from GitHub and Update](#3)\n",
    "    - [3.1 Ranking Data](#3_1)\n",
    "    - [3.2 Game Stats](#3_2)\n",
    "    - [3.3 Odds Data](#3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b002d-7571-40dd-9431-b4061e0d43af",
   "metadata": {},
   "source": [
    "## 1. Instructions <a id='1'></a>\n",
    "\n",
    "For Ranking Data:\n",
    "- Connect to GitHub in py file\n",
    "- Retrive data and add missing data\n",
    "- Add it back to original address\n",
    "\n",
    "For Game Stats:\n",
    "\n",
    "For Odds Data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3822d1-0412-4bce-b1be-be28d11a7350",
   "metadata": {},
   "source": [
    "## 2. Importing Functions and Packages <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b27480a3-c621-4922-848e-e4ced2ef3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.sessions import Session\n",
    "from nba_api.stats.endpoints import playergamelog\n",
    "from nba_api.stats.static import teams\n",
    "\n",
    "import base64\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.endpoints import playergamelog, leaguedashteamstats, teamyearbyyearstats\n",
    "from nba_api.stats.library.parameters import SeasonAll\n",
    "from nba_api.stats.static import players, teams\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import socks\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a7dbaf2a-a884-4856-880f-da010a345deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to maintain uniform naming schemes across datasets\n",
    "mapping = {'Brooklyn Nets': 'BRK', 'Golden State Warriors': 'GSW', 'Los Angeles Lakers': 'LAL',\n",
    "       'Milwaukee Bucks': 'MIL', 'Boston Celtics': 'BOS', 'Charlotte Hornets': 'CHO',\n",
    "       'Chicago Bulls': 'CHI', 'Cleveland Cavaliers': 'CLE', 'Denver Nuggets': 'DEN',\n",
    "       'Detroit Pistons': 'DET', 'Houston Rockets': 'HOU', 'Indiana Pacers': 'IND',\n",
    "       'Memphis Grizzlies': 'MEM', 'Minnesota Timberwolves': 'MIN',\n",
    "       'New Orleans Pelicans': 'NOP', 'New York Knicks': 'NYK', 'Oklahoma City Thunder': 'OKC',\n",
    "       'Orlando Magic': 'ORL', 'Philadelphia 76ers': 'PHI', 'Phoenix Suns': 'PHO',\n",
    "       'Portland Trail Blazers': 'POR', 'Sacramento Kings': 'SAC', 'San Antonio Spurs': 'SAS',\n",
    "       'Toronto Raptors': 'TOR', 'Utah Jazz': 'UTA', 'Washington Wizards': 'WAS',\n",
    "       'Atlanta Hawks': 'ATL', 'Dallas Mavericks': 'DAL', 'LA Clippers': 'LAC', 'Miami Heat': 'MIA'}\n",
    "\n",
    "def merge_with_suffixes(dataframes, names, keys):\n",
    "    \"\"\"Merging different sets of data of the same season together\"\"\"\n",
    "    suffixed_dfs = []\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Suffix non-key columns only\n",
    "        suffixed_cols = {col: f\"{col}_{name}\" if col not in keys else col for col in df.columns}\n",
    "        suffixed_dfs.append(df.rename(columns=suffixed_cols))\n",
    "\n",
    "    merged_df = suffixed_dfs[0]\n",
    "    for df in suffixed_dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=keys, how='inner')\n",
    "    return merged_df\n",
    "    \n",
    "\n",
    "def extracting_today_data(from_date=None, season='2023-24', season_type='Regular Season', data_type='Base', delay=5):\n",
    "    year_start = season[:4]\n",
    "    year_end = str(int(year_start)+1)\n",
    "    season_ = year_start + \"-\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "    if from_date == None:\n",
    "        path_today = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/base_{season}.csv\"\n",
    "        from_date = pd.read_csv(path_today, parse_dates=['Date'])['Date'].max().date()\n",
    "\n",
    "    season_end = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    \n",
    "    current_date = from_date\n",
    "    all_data = []\n",
    "\n",
    "    unsuccessful_dates = []\n",
    "    \n",
    "    while current_date <= season_end:\n",
    "        date_str = current_date.strftime('%m/%d/%Y')\n",
    "        \n",
    "        try:\n",
    "            daily_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "                measure_type_detailed_defense=data_type,\n",
    "                season=season_,\n",
    "                season_type_all_star=season_type,\n",
    "                date_to_nullable=date_str\n",
    "            ).get_data_frames()[0]\n",
    "            daily_stats['Date'] = date_str\n",
    "            all_data.append(daily_stats)\n",
    "            print(f\"Data fetched for {date_str}\")\n",
    "        except Exception as e:\n",
    "            unsuccessful_dates += [date_str]\n",
    "            print(f\"Error fetching data for {date_str}: {e}\")\n",
    "        time.sleep(delay)\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    full_season_data = pd.concat(all_data, ignore_index=True)\n",
    "    return full_season_data, unsuccessful_dates\n",
    "\n",
    "def getting_stats(delay=5, retrieving_from=None):\n",
    "    \"\"\"Getting cumulative season stats for each game\"\"\"\n",
    "    track_start='2023'\n",
    "    season_end='2024'\n",
    "    if retrieving_from is None:\n",
    "        retrieving_from = datetime.now() - timedelta(seconds=delay)\n",
    "    \n",
    "    base, unsuccessful_dates_base = extracting_today_data(from_date=retrieving_from, data_type='Base', delay=delay)\n",
    "    advanced, unsuccessful_dates_advanced = extracting_today_data(from_date=retrieving_from, data_type='Advanced', delay=delay)\n",
    "    misc, unsuccessful_dates_misc = extracting_today_data(from_date=retrieving_from, data_type='Misc', delay=delay)\n",
    "    four_factors, unsuccessful_dates_four_factors = extracting_today_data(from_date=retrieving_from, data_type='Four Factors', delay=delay)\n",
    "    scoring, unsuccessful_dates_scoring = extracting_today_data(from_date=retrieving_from, data_type='Scoring', delay=delay)\n",
    "    opponent, unsuccessful_dates_opponent = extracting_today_data(from_date=retrieving_from, data_type='Opponent', delay=delay)\n",
    "    defense, unsuccessful_dates_defense = extracting_today_data(from_date=retrieving_from, data_type='Defense', delay=delay)\n",
    "\n",
    "    year_start = track_start[:4]\n",
    "    year_end = season_end[:4]\n",
    "    season_ = year_start + \"_\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "\n",
    "    datas = [base, advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    \n",
    "    unsuccessful_dates_lst = [unsuccessful_dates_base, unsuccessful_dates_advanced, unsuccessful_dates_misc, unsuccessful_dates_four_factors, unsuccessful_dates_scoring, unsuccessful_dates_opponent, unsuccessful_dates_defense]\n",
    "    datas_names = [\"base\", \"advanced\", \"misc\", \"four_factors\",\n",
    "               \"scoring\", \"opponent\", \"defense\"]\n",
    "    columns_to_exclude = ['TEAM_NAME', 'GP', 'W', 'L', 'W_PCT', 'MIN']\n",
    "    others = [advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    others = [i[i.columns[~i.columns.isin(columns_to_exclude)]] for i in others]\n",
    "    datas = [base] + others\n",
    "    \n",
    "    merge_keys = ['Date', 'TEAM_ID']\n",
    "    merged_df = merge_with_suffixes(datas, datas_names, merge_keys) \n",
    "    merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def process(df):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['Teams'] = df['TEAM_NAME_base'].map(mapping)\n",
    "    del df['TEAM_NAME_base']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6c955-3863-466c-b236-b5e7a380bc7f",
   "metadata": {},
   "source": [
    "## 3. Retrieving Datasets from GitHub and Update <a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a84f4-a891-46d6-99ef-0f60aa96bb82",
   "metadata": {},
   "source": [
    "### 3.1 Ranking Data <a id='3_1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b8a106f6-a552-464a-81af-ec6e467f4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated already\n"
     ]
    }
   ],
   "source": [
    "# Your personal access token and repo details\n",
    "token = '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'\n",
    "username = 'bluefish0125'\n",
    "repo = 'Sports-Betting'\n",
    "path_to_file = 'nba_api/data/teams_stats/processed_cum_2018_2024.csv'\n",
    "\n",
    "# GitHub API URL for your file\n",
    "url = f'https://api.github.com/repos/{username}/{repo}/contents/{path_to_file}'\n",
    "\n",
    "# Hvae to use raw file\n",
    "raw_url = 'https://raw.githubusercontent.com/bluefish0125/Sports-Betting/main/nba_api/data/teams_stats/processed_cum_2018_2024.csv'\n",
    "# Send a GET request with headers including your personal access token for authentication\n",
    "response = requests.get(raw_url, headers=headers)\n",
    "if response.status_code == 200: # 200 means successful\n",
    "    processed_data = pd.read_csv(StringIO(response.text), index_col=0)\n",
    "    processed_data['Date'] = pd.to_datetime(processed_data['Date'], format='mixed')    \n",
    "    max_day = processed_data['Date'].max().date()\n",
    "    next_day = max_day + datetime.timedelta(days=1)\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve the CSV file. Status Code: {response.status_code}\")\n",
    "\n",
    "today = pd.DataFrame()\n",
    "if (next_day == datetime.date.today()):\n",
    "    print(\"Updated already\")\n",
    "else:\n",
    "    print(\"Getting Stats\")\n",
    "    print(next_day)\n",
    "    today = getting_stats(retrieving_from=next_day).iloc[:, 1:]\n",
    "    today = process(today)\n",
    "nba = pd.concat([processed_data, today], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7159b448-5520-40c3-bbd7-42e97ee8332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-04-05 00:00:00')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba['Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ef8ec141-99f6-4513-b743-9a89d466950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = nba[nba['Date'] < nba['Date'].max()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d2ac87c9-90b8-4e76-bc39-3fc4db06416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to CSV\n",
    "csv_content = nba.to_csv()\n",
    "content_encoded = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')\n",
    "new_path_to_file = 'nba_api/data/teams_stats/processed_cum_2018_2024.csv'\n",
    "# new_path_to_file = 'data_pipeline/test_file/test_file_1.csv'\n",
    "new_url = f'https://api.github.com/repos/{username}/{repo}/contents/{new_path_to_file}'\n",
    "# Prepare headers\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "# Fetch the file from GitHub to get its SHA\n",
    "response = requests.get(new_url, headers=headers)\n",
    "data = response.json()\n",
    "sha = data['sha']\n",
    "\n",
    "# Create the payload with the new content and the SHA\n",
    "payload = {\n",
    "    'message': 'Update CSV file',\n",
    "    'content': content_encoded,\n",
    "    'sha': sha,\n",
    "    'branch': 'main',  # specify the branch if not 'main'\n",
    "}\n",
    "\n",
    "# Make a PUT request to update the file\n",
    "response = requests.put(new_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('File updated successfully.')\n",
    "else:\n",
    "    print('Failed to update the file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008cc73-4ad6-46bf-9fc4-bce13cebc25d",
   "metadata": {},
   "source": [
    "### Modifying Data in GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "50606416-6e98-4dc7-8ca2-aeab8280e96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to CSV\n",
    "csv_content = nba.to_csv()\n",
    "content_encoded = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')\n",
    "new_path_to_file = 'nba_api/data/teams_stats/processed_cum_2018_2024.csv'\n",
    "new_url = f'https://api.github.com/repos/{username}/{repo}/contents/{new_path_to_file}'\n",
    "# Prepare headers\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "# Fetch the file from GitHub to get its SHA\n",
    "response = requests.get(new_url, headers=headers)\n",
    "data = response.json()\n",
    "sha = data['sha']\n",
    "\n",
    "# Create the payload with the new content and the SHA\n",
    "payload = {\n",
    "    'message': 'Update CSV file',\n",
    "    'content': content_encoded,\n",
    "    'sha': sha,\n",
    "    'branch': 'main',  # specify the branch if not 'main'\n",
    "}\n",
    "\n",
    "# Make a PUT request to update the file\n",
    "response = requests.put(url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('File updated successfully.')\n",
    "else:\n",
    "    print('Failed to update the file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b1bac321-4f28-4479-b272-dad121a30b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create the new file. Status Code: 422 Response: {'message': 'Invalid request.\\n\\n\"sha\" wasn\\'t supplied.', 'documentation_url': 'https://docs.github.com/rest/repos/contents#create-or-update-file-contents'}\n"
     ]
    }
   ],
   "source": [
    "csv_content = nba.to_csv()\n",
    "content_encoded = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')\n",
    "new_path_to_file = 'data_pipeline/test_file/test_file_1.csv'\n",
    "new_url = f'https://api.github.com/repos/{username}/{repo}/contents/{new_path_to_file}'\n",
    "\n",
    "# Prepare headers for authentication\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "# Create the payload with the new content\n",
    "payload = {\n",
    "    'message': 'Add new CSV file',\n",
    "    'content': content_encoded,\n",
    "    'branch': 'main',  # specify the branch if not 'main'\n",
    "}\n",
    "\n",
    "# Make a PUT request to upload the file\n",
    "response = requests.put(new_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print('New file created successfully.')\n",
    "else:\n",
    "    print(f'Failed to create the new file. Status Code: {response.status_code} Response: {response.json()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d600a1-43eb-4e14-a24e-d558d96d2434",
   "metadata": {},
   "source": [
    "### 3.2 Game Stats <a id='3_2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d65d25-0c63-41b0-9432-123d147b5bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98d05fb4-bca7-430d-b23d-2bccdafdb997",
   "metadata": {},
   "source": [
    "### 3.3 Odds Data <a id='3_3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c09030-f9f2-443e-9238-54551161d453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f350e-ff2e-406a-859e-08efef652028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bcd30-cda7-4418-aeae-cc1db292e0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ece16f-ee89-4174-a476-c1436e9ffb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b8d8b-3220-4aff-93fc-f4b887f70bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d083b-c565-4b71-a6fe-0358ff9618e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e05b6b-198e-4105-b9b9-fce358e3c64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48e8f4-ce05-46af-9601-6ff161b68982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83083ab-2a9e-4302-9224-484a7a464d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d55500-d8ef-488c-8421-1de4fdfb9d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d07c8fd-656d-4e0f-844f-a92f2fc13913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file created successfully.\n"
     ]
    }
   ],
   "source": [
    "csv_content = df.iloc[0, :].to_csv(index=False)\n",
    "content_encoded = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')\n",
    "new_path_to_file = 'data_pipeline/test_file/test_file_1.csv'\n",
    "new_url = f'https://api.github.com/repos/{username}/{repo}/contents/{new_path_to_file}'\n",
    "\n",
    "# Prepare headers for authentication\n",
    "headers = {\n",
    "    'Authorization': f'token {token}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "# Create the payload with the new content\n",
    "payload = {\n",
    "    'message': 'Add new CSV file',\n",
    "    'content': content_encoded,\n",
    "    'branch': 'main',  # specify the branch if not 'main'\n",
    "}\n",
    "\n",
    "# Make a PUT request to upload the file\n",
    "response = requests.put(new_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print('New file created successfully.')\n",
    "else:\n",
    "    print(f'Failed to create the new file. Status Code: {response.status_code} Response: {response.json()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7934f-4b61-4c39-a97c-2d547e15b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.endpoints import playergamelog, leaguedashteamstats, teamyearbyyearstats\n",
    "from nba_api.stats.library.parameters import SeasonAll\n",
    "from nba_api.stats.static import players, teams\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def merge_with_suffixes(dataframes, names, keys):\n",
    "    \"\"\"Merging different sets of data of the same season together\"\"\"\n",
    "    suffixed_dfs = []\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Suffix non-key columns only\n",
    "        suffixed_cols = {col: f\"{col}_{name}\" if col not in keys else col for col in df.columns}\n",
    "        suffixed_dfs.append(df.rename(columns=suffixed_cols))\n",
    "\n",
    "    merged_df = suffixed_dfs[0]\n",
    "    for df in suffixed_dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=keys, how='inner')\n",
    "    return merged_df\n",
    "    \n",
    "\n",
    "def extracting_today_data(from_date=None, season='2023-24', season_type='Regular Season', data_type='Base', delay=5):\n",
    "    year_start = season[:4]\n",
    "    year_end = str(int(year_start)+1)\n",
    "    season_ = year_start + \"-\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "    if from_date == None:\n",
    "        path_today = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/base_{season}.csv\"\n",
    "        from_date = pd.read_csv(path_today, parse_dates=['Date'])['Date'].max().date()\n",
    "\n",
    "    season_end = datetime.date.today()\n",
    "    \n",
    "    current_date = from_date\n",
    "    all_data = []\n",
    "\n",
    "    unsuccessful_dates = []\n",
    "    \n",
    "    while current_date <= season_end:\n",
    "        date_str = current_date.strftime('%m/%d/%Y')\n",
    "        \n",
    "        try:\n",
    "            daily_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "                measure_type_detailed_defense=data_type,\n",
    "                season=season_,\n",
    "                season_type_all_star=season_type,\n",
    "                date_to_nullable=date_str\n",
    "            ).get_data_frames()[0]\n",
    "            daily_stats['Date'] = date_str\n",
    "            all_data.append(daily_stats)\n",
    "            print(f\"Data fetched for {date_str}\")\n",
    "        except Exception as e:\n",
    "            unsuccessful_dates += [date_str]\n",
    "            print(f\"Error fetching data for {date_str}: {e}\")\n",
    "        time.sleep(delay)\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    full_season_data = pd.concat(all_data, ignore_index=True)\n",
    "    return full_season_data, unsuccessful_dates\n",
    "\n",
    "def getting_stats(delay=5):\n",
    "    \"\"\"Getting cumulative season stats for each game\"\"\"\n",
    "    track_start='2023'\n",
    "    season_end='2024'\n",
    "    \n",
    "    base, unsuccessful_dates_base = extracting_today_data(data_type='Base', delay=delay)\n",
    "    advanced, unsuccessful_dates_advanced = extracting_today_data(data_type='Advanced', delay=delay)\n",
    "    misc, unsuccessful_dates_misc = extracting_today_data(data_type='Misc', delay=delay)\n",
    "    four_factors, unsuccessful_dates_four_factors = extracting_today_data(data_type='Four Factors', delay=delay)\n",
    "    scoring, unsuccessful_dates_scoring = extracting_today_data(data_type='Scoring', delay=delay)\n",
    "    opponent, unsuccessful_dates_opponent = extracting_today_data(data_type='Opponent', delay=delay)\n",
    "    defense, unsuccessful_dates_defense = extracting_today_data(data_type='Defense', delay=delay)\n",
    "\n",
    "    year_start = track_start[:4]\n",
    "    year_end = season_end[:4]\n",
    "    season_ = year_start + \"_\" + year_end[2:]\n",
    "    season = year_start + \"_\" + year_end\n",
    "\n",
    "    datas = [base, advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    unsuccessful_dates_lst = [unsuccessful_dates_base, unsuccessful_dates_advanced, unsuccessful_dates_misc, unsuccessful_dates_four_factors, unsuccessful_dates_scoring, unsuccessful_dates_opponent, unsuccessful_dates_defense]\n",
    "    datas_names = [\"base\", \"advanced\", \"misc\", \"four_factors\",\n",
    "               \"scoring\", \"opponent\", \"defense\"]\n",
    "    columns_to_exclude = ['TEAM_NAME', 'GP', 'W', 'L', 'W_PCT', 'MIN']\n",
    "    others = [advanced, misc, four_factors, scoring, opponent, defense]\n",
    "    others = [i[i.columns[~i.columns.isin(columns_to_exclude)]] for i in others]\n",
    "    datas = [base] + others\n",
    "    \n",
    "    merge_keys = ['Date', 'TEAM_ID']\n",
    "    merged_df = merge_with_suffixes(datas, datas_names, merge_keys)    \n",
    "    latest_path = f\"/Users/liqingyang/Documents/GitHub/sports_trading/sports_betting/nba_api/data/teams_stats/{season}/cumulative_season_stats_{season}.csv\"\n",
    "    prev = pd.read_csv(latest_path, parse_dates=['Date'])\n",
    "    merged_df = pd.concat([prev, merged_df], ignore_index=True)\n",
    "    merged_df.to_csv(latest_path, index=False)\n",
    "    return merged_df\n",
    "\n",
    "today = getting_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
